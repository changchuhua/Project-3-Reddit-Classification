{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Imports](#import)\n",
    "- [Punctuation and Numeric Values](#clean)\n",
    "- [Models](#model)\n",
    "- [Tf-Idf](#tfidf)\n",
    "- [Max Features](#maxft)\n",
    "- [2-gram Count Vectorization](#2gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports<a id=import></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the csv created from the earlier steps in our process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned=pd.read_csv(r'..\\datasets\\cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Punctuation and Numeric Values<a id=clean></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which allows us to specify if we wish to clean punctuation and numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def furtherclean(df,feature,stop_words=True,digits=True,punct=True):\n",
    "    df1=df.copy()\n",
    "    df1[feature]=df1['text']\n",
    "    # clean with stop words\n",
    "    if stop_words==True:\n",
    "        df1[feature]=df1[feature].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    # drop numeric values\n",
    "    if digits==True:\n",
    "        df1[feature]=df1[feature].str.replace('\\d+', ' ')\n",
    "    # drop all punctuation\n",
    "    if punct==True:\n",
    "        df1[feature]=df1[feature].str.replace(r'[^\\s\\w]+', ' ')\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create 4 DataFrames with different forms of cleaning and check their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stop=furtherclean(cleaned,'stoptext',digits=False,punct=False) # only stop words are removed\n",
    "cleaned_stoppunct=furtherclean(cleaned,'stoptext',digits=False) # stop words and punctuation are removed\n",
    "cleaned_stopdigit=furtherclean(cleaned,'stoptext',punct=False) # stop words and numeric values are removed\n",
    "cleaned_all=furtherclean(cleaned,'stoptext') # stop words, punctuation and numeric values are removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned: does pressure in a sealed container rise as it ascends in altitude?the source of this discussion is talking about inflating an inflatable stand up paddleboard at lower altitude and then driving it up to a mountain lake. these paddle boards have a stiff strong structure that holds their shape. they are designed to hold aprox. 15psi. if someone was to fill the paddleboard to 15psi say at 4,000ft altitude, then drive it to a mountain lake at say 8,000ft altitude, will the pressure in the paddleboard change from the change in altitude or is 15psi in a container, 15psi regardless of ambient pressure?\n",
      "\n",
      "cleaned_stop: pressure sealed container rise ascends altitude?the source discussion talking inflating inflatable stand paddleboard lower altitude driving mountain lake. paddle boards stiff strong structure holds shape. designed hold aprox. 15psi. someone fill paddleboard 15psi say 4,000ft altitude, drive mountain lake say 8,000ft altitude, pressure paddleboard change change altitude 15psi container, 15psi regardless ambient pressure?\n",
      "\n",
      "cleaned_stoppunct: pressure sealed container rise ascends altitude the source discussion talking inflating inflatable stand paddleboard lower altitude driving mountain lake  paddle boards stiff strong structure holds shape  designed hold aprox  15psi  someone fill paddleboard 15psi say 4 000ft altitude  drive mountain lake say 8 000ft altitude  pressure paddleboard change change altitude 15psi container  15psi regardless ambient pressure \n",
      "\n",
      "cleaned_stopdigit: pressure sealed container rise ascends altitude?the source discussion talking inflating inflatable stand paddleboard lower altitude driving mountain lake. paddle boards stiff strong structure holds shape. designed hold aprox.  psi. someone fill paddleboard  psi say  , ft altitude, drive mountain lake say  , ft altitude, pressure paddleboard change change altitude  psi container,  psi regardless ambient pressure?\n",
      "\n",
      "cleaned_all: pressure sealed container rise ascends altitude the source discussion talking inflating inflatable stand paddleboard lower altitude driving mountain lake  paddle boards stiff strong structure holds shape  designed hold aprox   psi  someone fill paddleboard  psi say    ft altitude  drive mountain lake say    ft altitude  pressure paddleboard change change altitude  psi container   psi regardless ambient pressure \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('cleaned: ' + cleaned.text[5]+'\\n')\n",
    "print('cleaned_stop: ' + cleaned_stop.stoptext[5]+'\\n')\n",
    "print('cleaned_stoppunct: ' + cleaned_stoppunct.stoptext[5]+'\\n')\n",
    "print('cleaned_stopdigit: ' + cleaned_stopdigit.stoptext[5]+'\\n')\n",
    "print('cleaned_all: ' + cleaned_all.stoptext[5]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our cleaning has taken place without any issues for the 4 dataframes.<br/>\n",
    "\n",
    "Next, we create a function what will perform count vecorization and score a logistic regression on whatever dataframe we send as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logregr(df,framename):\n",
    "    logreg=LogisticRegression()\n",
    "    cvec = CountVectorizer(stop_words='english',strip_accents='unicode')\n",
    "    X_train,X_test,y_train,y_test=train_test_split(df.stoptext,df.subreddit,random_state=42,stratify=df.subreddit)\n",
    "    X_train=pd.DataFrame(cvec.fit_transform(X_train).todense(),columns=cvec.get_feature_names())\n",
    "    X_test=pd.DataFrame(cvec.transform(X_test).todense(),columns=cvec.get_feature_names())\n",
    "    logreg.fit(X_train,y_train)\n",
    "    score=logreg.score(X_test,y_test)\n",
    "    print('The score for '+framename+' is '+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for cleaned_stop is 0.8734939759036144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for cleaned_stoppunct is 0.8734939759036144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for cleaned_stopdigit is 0.8714859437751004\n",
      "The score for cleaned_all is 0.8714859437751004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logregr(cleaned_stop,'cleaned_stop')\n",
    "logregr(cleaned_stoppunct,'cleaned_stoppunct')\n",
    "logregr(cleaned_stopdigit,'cleaned_stopdigit')\n",
    "logregr(cleaned_all,'cleaned_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be minimal difference in the methods of cleaning.<br/>\n",
    "Following the logic that digits and punctuation should not be part of our classifier, we will utilize the 'cleaned_all' dataframe for the following model tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Models<a id=model></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code has been adapted from David S. Batista from his [blog](http://www.davidsbatista.net/blog/2018/02/23/model_optimization/).<br/>\n",
    "This class is written to allow us to perform a GridSearch on a predefined dictionary of models and their parameters and output their scores in a nicely formatted dataframe.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code found from http://www.davidsbatista.net/blog/2018/02/23/model_optimization/\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=-1, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator': key,\n",
    "                 'min_score': min(scores),\n",
    "                 'max_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_models = {\n",
    "    'LogisticRegression' : LogisticRegression(random_state = 42),\n",
    "    'KNN': KNeighborsClassifier(), \n",
    "    'NaiveBayes' : MultinomialNB(),\n",
    "    'DecisionTree' : DecisionTreeClassifier(random_state = 42), \n",
    "    'BaggedDecisionTree' : BaggingClassifier(random_state = 42),\n",
    "    'RandomForest' : RandomForestClassifier(random_state = 42), \n",
    "    'ExtraTrees' : ExtraTreesClassifier(random_state = 42), \n",
    "    'AdaBoost' : AdaBoostClassifier(random_state=42), \n",
    "    'GradientBoosting' : GradientBoostingClassifier(random_state = 42),\n",
    "    'SVM' : SVC(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model_params = {\n",
    "    'LogisticRegression' : {\n",
    "        'penalty' : ['l1', 'l2'],\n",
    "        'C' : np.arange(.05, 1, .05) },\n",
    "    'KNN' : {\n",
    "        'n_neighbors' : np.arange(3, 22, 2) },\n",
    "    'NaiveBayes' : {\n",
    "        'alpha' : np.arange(.05, 2, .05)},\n",
    "    'DecisionTree': {\n",
    "        'max_depth' : [None, 6, 10, 14], \n",
    "        'min_samples_leaf' : [1, 2],\n",
    "        'min_samples_split': [2, 3] },\n",
    "    'BaggedDecisionTree' : {\n",
    "        'n_estimators' : [20, 60, 100] },\n",
    "    'RandomForest' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 2, 6, 10],\n",
    "        'min_samples_split' : [2, 3, 4] },\n",
    "    'ExtraTrees' : {\n",
    "        'n_estimators' : [20, 60, 100],\n",
    "        'max_depth' : [None, 6, 10, 14],\n",
    "        'min_samples_leaf' : [1, 2], \n",
    "        'min_samples_split' : [2, 3], },\n",
    "    'AdaBoost' : {\n",
    "        'n_estimators' : np.arange(100, 151, 25),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10) },\n",
    "    'GradientBoosting' : {\n",
    "        'n_estimators' : np.arange(5, 150, 15),\n",
    "        'learning_rate' : np.linspace(0.05, 1, 10),\n",
    "        'max_depth' : [1, 2, 3] },\n",
    "    'SVM' : {\n",
    "        'C' : np.arange(0.05, 1, .05),\n",
    "        'kernel' : ['rbf', 'linear'] },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count vectorize our 'cleaned_all' dataframe,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=cleaned_all\n",
    "cvec = CountVectorizer(stop_words='english',strip_accents='unicode')\n",
    "X_train,X_test,y_train,y_test=train_test_split(df.stoptext,df.subreddit,random_state=42,stratify=df.subreddit)\n",
    "X_train=pd.DataFrame(cvec.fit_transform(X_train).todense(),columns=cvec.get_feature_names())\n",
    "X_test=pd.DataFrame(cvec.transform(X_test).todense(),columns=cvec.get_feature_names())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we perform our GridSearchCV model fittings.<br/>\n",
    "We use the 'f1' scoring method to grade the precision and recall of our classification models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:   15.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for KNN.\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for NaiveBayes.\n",
      "Fitting 3 folds for each of 39 candidates, totalling 117 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 117 out of 117 | elapsed:   10.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for DecisionTree.\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:   24.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for BaggedDecisionTree.\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  1.2min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for RandomForest.\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   48.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for ExtraTrees.\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:  2.8min finished\n"
     ]
    }
   ],
   "source": [
    "search = EstimatorSelectionHelper(classifier_models, classifier_model_params)\n",
    "search.fit(X_train, y_train, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then score our different models and output our results to a csv for archival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "KNN\n",
      "NaiveBayes\n",
      "DecisionTree\n",
      "BaggedDecisionTree\n",
      "RandomForest\n",
      "ExtraTrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score1=search.score_summary(sort_by='max_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1.to_csv(r'..\\datasets\\score1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to read in the different model scores on future runs\n",
    "score1=pd.read_csv(r'..\\datasets\\score1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will sort our scores according to the highest mean 'f1' scores obtained for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>alpha</th>\n",
       "      <th>estimator</th>\n",
       "      <th>kernel</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_score</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "      <th>std_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.15</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.912368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.899804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900862</td>\n",
       "      <td>0.872827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.847682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.021807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894410</td>\n",
       "      <td>0.874189</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.851613</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.879650</td>\n",
       "      <td>0.864767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.849673</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BaggedDecisionTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.858388</td>\n",
       "      <td>0.842160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827133</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.802633</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.794355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KNN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.438272</td>\n",
       "      <td>0.390985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872017</td>\n",
       "      <td>0.848122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828194</td>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870690</td>\n",
       "      <td>0.850494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.836910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.842022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.836689</td>\n",
       "      <td>110.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C  alpha           estimator  kernel  learning_rate  max_depth  \\\n",
       "2     NaN   1.15          NaiveBayes     NaN            NaN        NaN   \n",
       "39   0.05    NaN  LogisticRegression     NaN            NaN        NaN   \n",
       "45    NaN    NaN          ExtraTrees     NaN            NaN        NaN   \n",
       "66    NaN    NaN        RandomForest     NaN            NaN        NaN   \n",
       "78    NaN    NaN  BaggedDecisionTree     NaN            NaN        NaN   \n",
       "103   NaN    NaN        DecisionTree     NaN            NaN        NaN   \n",
       "180   NaN    NaN                 KNN     NaN            NaN        NaN   \n",
       "190   NaN    NaN            AdaBoost     NaN       0.577778        NaN   \n",
       "194  0.70    NaN                 SVM  linear            NaN        NaN   \n",
       "241   NaN    NaN    GradientBoosting     NaN       0.366667        3.0   \n",
       "\n",
       "     max_score  mean_score  min_samples_leaf  min_samples_split  min_score  \\\n",
       "2     0.924603    0.912368               NaN                NaN   0.899804   \n",
       "39    0.900862    0.872827               NaN                NaN   0.847682   \n",
       "45    0.894410    0.874189               1.0                2.0   0.851613   \n",
       "66    0.879650    0.864767               NaN                4.0   0.849673   \n",
       "78    0.858388    0.842160               NaN                NaN   0.827133   \n",
       "103   0.810127    0.802633               1.0                2.0   0.794355   \n",
       "180   0.438272    0.390985               NaN                NaN   0.355263   \n",
       "190   0.872017    0.848122               NaN                NaN   0.828194   \n",
       "194   0.870690    0.850494               NaN                NaN   0.836910   \n",
       "241   0.848214    0.842022               NaN                NaN   0.836689   \n",
       "\n",
       "     n_estimators  n_neighbors penalty  std_score  \n",
       "2             NaN          NaN     NaN   0.010127  \n",
       "39            NaN          NaN      l2   0.021807  \n",
       "45           60.0          NaN     NaN   0.017551  \n",
       "66          100.0          NaN     NaN   0.012239  \n",
       "78          100.0          NaN     NaN   0.012788  \n",
       "103           NaN          NaN     NaN   0.006463  \n",
       "180           NaN          3.0     NaN   0.034861  \n",
       "190         125.0          NaN     NaN   0.018109  \n",
       "194           NaN          NaN     NaN   0.014562  \n",
       "241         110.0          NaN     NaN   0.004744  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1[score1['mean_score'] == score1.groupby('estimator')['mean_score'].transform('max')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the Multinomial Naive Bayes model gives us the best mean f1 score at 0.91 as opposed to the other models.<br/>\n",
    "In general, only the Multinomial (max f1=0.92), Logistic Regression (max f1=0.90) and Extra Trees model (max f1=0.89) performed better than our baseline model with an f1 score of 0.889.<br/>\n",
    "The rest of the models still performed admirably well with the f1 scores sitting above 0.79, except for the k-Nearest Neighbouts model which gave us an absymal best score of 0.43.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8988326848249028"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb=MultinomialNB(alpha=1.15)\n",
    "mnb.fit(X_train,y_train)\n",
    "pred=mnb.predict(X_test)\n",
    "f1_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on our test set, our Multinomial classifier with an alpha parameter of 1.15 gives us an f1 score of 0.89 which beats our baseline by 0.01.<br/>\n",
    "MultinomialNB is our best classifier though the difference  from our baseline Logistic Regression classifier is minimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tf-Idf <a id=tfidf></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate if weighting our tokens with the *term frequency.inverse document frequency* method will yield any significant improvements to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=cleaned_all\n",
    "tvec = TfidfVectorizer(stop_words='english',strip_accents='unicode')\n",
    "X_train,X_test,y_train,y_test=train_test_split(df.stoptext,df.subreddit,random_state=42,stratify=df.subreddit)\n",
    "X_train=pd.DataFrame(tvec.fit_transform(X_train).todense(),columns=tvec.get_feature_names())\n",
    "X_test=pd.DataFrame(tvec.transform(X_test).todense(),columns=tvec.get_feature_names())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a GridSearch with all our different classifiers again,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for AdaBoost.\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  9.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GradientBoosting.\n",
      "Fitting 3 folds for each of 300 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 43.8min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 118.4min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 900 | elapsed: 132.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for SVM.\n",
      "Fitting 3 folds for each of 38 candidates, totalling 114 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed: 13.7min finished\n"
     ]
    }
   ],
   "source": [
    "search = EstimatorSelectionHelper(classifier_models, classifier_model_params)\n",
    "search.fit(X_train, y_train, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we archive our GridSearch f1 scores externally in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2=search.score_summary(sort_by='max_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2.to_csv(r'..\\datasets\\score2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to read in the different model scores on future runs\n",
    "score2=pd.read_csv(r'..\\datasets\\score2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>C</th>\n",
       "      <th>alpha</th>\n",
       "      <th>estimator</th>\n",
       "      <th>kernel</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_score</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>penalty</th>\n",
       "      <th>std_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.60</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>0.916491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.55</td>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>0.916491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>37</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.911340</td>\n",
       "      <td>0.900907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.883817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.012182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>33</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.911340</td>\n",
       "      <td>0.900907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.883817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.012182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>35</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.911340</td>\n",
       "      <td>0.900907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.883817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.012182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.886625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.868085</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>0.886625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.868085</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.863190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.852747</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KNN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.833795</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BaggedDecisionTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844639</td>\n",
       "      <td>0.831524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.823266</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.786469</td>\n",
       "      <td>0.780410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.768898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.786469</td>\n",
       "      <td>0.780410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.768898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>357</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.898965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.882845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.859649</td>\n",
       "      <td>0.842147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817778</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845494</td>\n",
       "      <td>0.835609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.821505</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     C  alpha           estimator  kernel  learning_rate  \\\n",
       "5            79   NaN   1.60          NaiveBayes     NaN            NaN   \n",
       "8            78   NaN   1.55          NaiveBayes     NaN            NaN   \n",
       "39           37  0.95    NaN  LogisticRegression     NaN            NaN   \n",
       "40           33  0.85    NaN  LogisticRegression     NaN            NaN   \n",
       "41           35  0.90    NaN  LogisticRegression     NaN            NaN   \n",
       "59          153   NaN    NaN          ExtraTrees     NaN            NaN   \n",
       "60          150   NaN    NaN          ExtraTrees     NaN            NaN   \n",
       "73          114   NaN    NaN        RandomForest     NaN            NaN   \n",
       "86           47   NaN    NaN                 KNN     NaN            NaN   \n",
       "101         105   NaN    NaN  BaggedDecisionTree     NaN            NaN   \n",
       "142          87   NaN    NaN        DecisionTree     NaN            NaN   \n",
       "143          88   NaN    NaN        DecisionTree     NaN            NaN   \n",
       "191         357  0.70    NaN                 SVM  linear            NaN   \n",
       "208         165   NaN    NaN    GradientBoosting     NaN       0.472222   \n",
       "243          11   NaN    NaN            AdaBoost     NaN       0.366667   \n",
       "\n",
       "     max_depth  max_score  mean_score  min_samples_leaf  min_samples_split  \\\n",
       "5          NaN   0.921212    0.916491               NaN                NaN   \n",
       "8          NaN   0.921212    0.916491               NaN                NaN   \n",
       "39         NaN   0.911340    0.900907               NaN                NaN   \n",
       "40         NaN   0.911340    0.900907               NaN                NaN   \n",
       "41         NaN   0.911340    0.900907               NaN                NaN   \n",
       "59         NaN   0.899160    0.886625               2.0                3.0   \n",
       "60         NaN   0.899160    0.886625               2.0                2.0   \n",
       "73         NaN   0.869565    0.863190               NaN                4.0   \n",
       "86         NaN   0.858333    0.833795               NaN                NaN   \n",
       "101        NaN   0.844639    0.831524               NaN                NaN   \n",
       "142        NaN   0.786469    0.780410               1.0                2.0   \n",
       "143        NaN   0.786469    0.780410               1.0                3.0   \n",
       "191        NaN   0.909091    0.898965               NaN                NaN   \n",
       "208        2.0   0.859649    0.842147               NaN                NaN   \n",
       "243        NaN   0.845494    0.835609               NaN                NaN   \n",
       "\n",
       "     min_score  n_estimators  n_neighbors penalty  std_score  \n",
       "5     0.909449           NaN          NaN     NaN   0.005075  \n",
       "8     0.909449           NaN          NaN     NaN   0.005075  \n",
       "39    0.883817           NaN          NaN      l2   0.012182  \n",
       "40    0.883817           NaN          NaN      l2   0.012182  \n",
       "41    0.883817           NaN          NaN      l2   0.012182  \n",
       "59    0.868085         100.0          NaN     NaN   0.013378  \n",
       "60    0.868085         100.0          NaN     NaN   0.013378  \n",
       "73    0.852747         100.0          NaN     NaN   0.007444  \n",
       "86    0.806452           NaN         21.0     NaN   0.021273  \n",
       "101   0.823266         100.0          NaN     NaN   0.009377  \n",
       "142   0.768898           NaN          NaN     NaN   0.008144  \n",
       "143   0.768898           NaN          NaN     NaN   0.008144  \n",
       "191   0.882845           NaN          NaN     NaN   0.011523  \n",
       "208   0.817778          80.0          NaN     NaN   0.017770  \n",
       "243   0.821505         150.0          NaN     NaN   0.010237  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2[score2['mean_score'] == score2.groupby('estimator')['mean_score'].transform('max')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon sorting our models by the highest mean score, we observe that the Multinomial Naive Bayes classifier still gives us the best results as expected.<br/>\n",
    "However, the f1 score did not have any significant improvement with the mean score still sitting at 0.91.<br/>\n",
    "We will use the best alpha parameter of 1.60 for our MultinomialNB to predict on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8949416342412451"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb=MultinomialNB(alpha=1.60)\n",
    "mnb.fit(X_train,y_train)\n",
    "pred=mnb.predict(X_test)\n",
    "f1_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on our test set, our f1 score actually drops to 0.894 though the drop is minimal. <br/>\n",
    "With worse results in our Tf-Idf vectorizer score, we choose to use count vectorization for all following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Max Features<a id=maxft></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we attempt to limit our maximum features (tokens) to 1000, from 7000+ features.<br/>\n",
    "We investigate if limiting the amount of features cause any significant detriment to the predictive power of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=cleaned_all\n",
    "cvec = CountVectorizer(stop_words='english',strip_accents='unicode',max_features=1000) #limit maximum features to 1000\n",
    "X_train,X_test,y_train,y_test=train_test_split(df.stoptext,df.subreddit,random_state=42,stratify=df.subreddit)\n",
    "X_train=pd.DataFrame(cvec.fit_transform(X_train).todense(),columns=cvec.get_feature_names())\n",
    "X_test=pd.DataFrame(cvec.transform(X_test).todense(),columns=cvec.get_feature_names())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a GridSearch with only the Multinomial Naive Bayes classifier here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv=GridSearchCV(MultinomialNB(),classifier_model_params['NaiveBayes'],scoring='f1',n_jobs=-1,verbose=1,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 39 candidates, totalling 117 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 117 out of 117 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  , 1.05, 1.1 ,\n",
       "       1.15, 1.2 , 1.25, 1.3 , 1.35, 1.4 , 1.45, 1.5 , 1.55, 1.6 , 1.65,\n",
       "       1.7 , 1.75, 1.8 , 1.85, 1.9 , 1.95])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.038894</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>0.011313</td>\n",
       "      <td>1.35</td>\n",
       "      <td>{'alpha': 1.35}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.900494</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>1</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.937120</td>\n",
       "      <td>0.943302</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.032581</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>1.4</td>\n",
       "      <td>{'alpha': 1.4000000000000001}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.900494</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>1</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.937120</td>\n",
       "      <td>0.943302</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.011753</td>\n",
       "      <td>0.009974</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1.45</td>\n",
       "      <td>{'alpha': 1.4500000000000002}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.900494</td>\n",
       "      <td>0.008519</td>\n",
       "      <td>1</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.937120</td>\n",
       "      <td>0.943302</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.031913</td>\n",
       "      <td>0.002935</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>1.55</td>\n",
       "      <td>{'alpha': 1.55}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.886680</td>\n",
       "      <td>0.903093</td>\n",
       "      <td>0.899625</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>4</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.934010</td>\n",
       "      <td>0.942266</td>\n",
       "      <td>0.006796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.041887</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.017620</td>\n",
       "      <td>0.009152</td>\n",
       "      <td>1.65</td>\n",
       "      <td>{'alpha': 1.6500000000000001}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.886680</td>\n",
       "      <td>0.903093</td>\n",
       "      <td>0.899625</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>4</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.934010</td>\n",
       "      <td>0.941947</td>\n",
       "      <td>0.006817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.041890</td>\n",
       "      <td>0.008810</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>1.6</td>\n",
       "      <td>{'alpha': 1.6}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.886680</td>\n",
       "      <td>0.903093</td>\n",
       "      <td>0.899625</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>4</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.934010</td>\n",
       "      <td>0.941947</td>\n",
       "      <td>0.006817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.045211</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>1.8</td>\n",
       "      <td>{'alpha': 1.8}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.884462</td>\n",
       "      <td>0.904959</td>\n",
       "      <td>0.899507</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>7</td>\n",
       "      <td>0.940102</td>\n",
       "      <td>0.949597</td>\n",
       "      <td>0.931841</td>\n",
       "      <td>0.940513</td>\n",
       "      <td>0.007254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.023271</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>1.95</td>\n",
       "      <td>{'alpha': 1.9500000000000002}</td>\n",
       "      <td>0.911290</td>\n",
       "      <td>0.882236</td>\n",
       "      <td>0.904959</td>\n",
       "      <td>0.899499</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>8</td>\n",
       "      <td>0.937945</td>\n",
       "      <td>0.950555</td>\n",
       "      <td>0.931841</td>\n",
       "      <td>0.940114</td>\n",
       "      <td>0.007792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.034575</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.95</td>\n",
       "      <td>{'alpha': 0.9500000000000001}</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.905738</td>\n",
       "      <td>0.899454</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>9</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.941414</td>\n",
       "      <td>0.945411</td>\n",
       "      <td>0.004446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.038901</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>1.3</td>\n",
       "      <td>{'alpha': 1.3}</td>\n",
       "      <td>0.906883</td>\n",
       "      <td>0.887129</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.899170</td>\n",
       "      <td>0.008626</td>\n",
       "      <td>10</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.937120</td>\n",
       "      <td>0.943302</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.040886</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'alpha': 0.6500000000000001}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.899099</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>11</td>\n",
       "      <td>0.944276</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>0.946511</td>\n",
       "      <td>0.003688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.038563</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'alpha': 0.7000000000000001}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.899099</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>11</td>\n",
       "      <td>0.944276</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>0.946511</td>\n",
       "      <td>0.003688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.033909</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'alpha': 0.7500000000000001}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.899099</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>11</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.942482</td>\n",
       "      <td>0.945447</td>\n",
       "      <td>0.003694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.033576</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'alpha': 1.5000000000000002}</td>\n",
       "      <td>0.906883</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.899006</td>\n",
       "      <td>0.007516</td>\n",
       "      <td>14</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.936041</td>\n",
       "      <td>0.942942</td>\n",
       "      <td>0.005994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.035239</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'alpha': 0.9000000000000001}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.905738</td>\n",
       "      <td>0.898844</td>\n",
       "      <td>0.008035</td>\n",
       "      <td>15</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.942482</td>\n",
       "      <td>0.945767</td>\n",
       "      <td>0.004144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.043217</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>0.014627</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>1.75</td>\n",
       "      <td>{'alpha': 1.7500000000000002}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.882236</td>\n",
       "      <td>0.904959</td>\n",
       "      <td>0.898764</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>16</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.949597</td>\n",
       "      <td>0.934010</td>\n",
       "      <td>0.941594</td>\n",
       "      <td>0.006370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.041555</td>\n",
       "      <td>0.010936</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>1.85</td>\n",
       "      <td>{'alpha': 1.85}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.882236</td>\n",
       "      <td>0.904959</td>\n",
       "      <td>0.898764</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>16</td>\n",
       "      <td>0.940102</td>\n",
       "      <td>0.949597</td>\n",
       "      <td>0.931841</td>\n",
       "      <td>0.940513</td>\n",
       "      <td>0.007254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.026597</td>\n",
       "      <td>0.008708</td>\n",
       "      <td>0.006316</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>1.9</td>\n",
       "      <td>{'alpha': 1.9000000000000001}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.882236</td>\n",
       "      <td>0.904959</td>\n",
       "      <td>0.898764</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>16</td>\n",
       "      <td>0.940102</td>\n",
       "      <td>0.949597</td>\n",
       "      <td>0.931841</td>\n",
       "      <td>0.940513</td>\n",
       "      <td>0.007254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.034242</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>1.05</td>\n",
       "      <td>{'alpha': 1.05}</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.898706</td>\n",
       "      <td>0.007897</td>\n",
       "      <td>19</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.941414</td>\n",
       "      <td>0.945411</td>\n",
       "      <td>0.004446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.034575</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.898706</td>\n",
       "      <td>0.007897</td>\n",
       "      <td>19</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.941414</td>\n",
       "      <td>0.945411</td>\n",
       "      <td>0.004446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.036568</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.009308</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.85</td>\n",
       "      <td>{'alpha': 0.8500000000000001}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.904277</td>\n",
       "      <td>0.898358</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>21</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.942482</td>\n",
       "      <td>0.945767</td>\n",
       "      <td>0.004144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.044552</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'alpha': 0.8}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.904277</td>\n",
       "      <td>0.898358</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>21</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.942482</td>\n",
       "      <td>0.945447</td>\n",
       "      <td>0.003694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.037233</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>1.7</td>\n",
       "      <td>{'alpha': 1.7000000000000002}</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.882236</td>\n",
       "      <td>0.903093</td>\n",
       "      <td>0.898144</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>23</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.949597</td>\n",
       "      <td>0.934010</td>\n",
       "      <td>0.941594</td>\n",
       "      <td>0.006370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.036237</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>1.1</td>\n",
       "      <td>{'alpha': 1.1}</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.898124</td>\n",
       "      <td>0.008719</td>\n",
       "      <td>24</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.944696</td>\n",
       "      <td>0.005148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.036895</td>\n",
       "      <td>0.003554</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>1.25</td>\n",
       "      <td>{'alpha': 1.2500000000000002}</td>\n",
       "      <td>0.906883</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.898003</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>25</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.938197</td>\n",
       "      <td>0.943661</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.009817</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'alpha': 0.6000000000000001}</td>\n",
       "      <td>0.901010</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.897777</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>26</td>\n",
       "      <td>0.944276</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>0.946511</td>\n",
       "      <td>0.003688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054188</td>\n",
       "      <td>0.006003</td>\n",
       "      <td>0.010307</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'alpha': 0.2}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.885437</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.897425</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>27</td>\n",
       "      <td>0.946411</td>\n",
       "      <td>0.953815</td>\n",
       "      <td>0.945674</td>\n",
       "      <td>0.948633</td>\n",
       "      <td>0.003677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.037233</td>\n",
       "      <td>0.004018</td>\n",
       "      <td>0.009974</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>1.15</td>\n",
       "      <td>{'alpha': 1.1500000000000001}</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.897391</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>28</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>0.944696</td>\n",
       "      <td>0.005148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.036235</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{'alpha': 1.2000000000000002}</td>\n",
       "      <td>0.905051</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.903491</td>\n",
       "      <td>0.897391</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>28</td>\n",
       "      <td>0.942132</td>\n",
       "      <td>0.950655</td>\n",
       "      <td>0.938197</td>\n",
       "      <td>0.943661</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.048869</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.45</td>\n",
       "      <td>{'alpha': 0.45}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.884086</td>\n",
       "      <td>0.904665</td>\n",
       "      <td>0.897325</td>\n",
       "      <td>0.009379</td>\n",
       "      <td>30</td>\n",
       "      <td>0.945344</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.944612</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.034907</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'alpha': 0.55}</td>\n",
       "      <td>0.901010</td>\n",
       "      <td>0.885827</td>\n",
       "      <td>0.904665</td>\n",
       "      <td>0.897165</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>31</td>\n",
       "      <td>0.945344</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>0.946868</td>\n",
       "      <td>0.003502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.036565</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.25</td>\n",
       "      <td>{'alpha': 0.25}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.883268</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.896703</td>\n",
       "      <td>0.009501</td>\n",
       "      <td>32</td>\n",
       "      <td>0.946411</td>\n",
       "      <td>0.952764</td>\n",
       "      <td>0.945674</td>\n",
       "      <td>0.948283</td>\n",
       "      <td>0.003183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.009974</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>0.901010</td>\n",
       "      <td>0.884086</td>\n",
       "      <td>0.904665</td>\n",
       "      <td>0.896585</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>33</td>\n",
       "      <td>0.945344</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>0.946868</td>\n",
       "      <td>0.003502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039227</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.882012</td>\n",
       "      <td>0.906188</td>\n",
       "      <td>0.896533</td>\n",
       "      <td>0.010452</td>\n",
       "      <td>34</td>\n",
       "      <td>0.948537</td>\n",
       "      <td>0.952859</td>\n",
       "      <td>0.948847</td>\n",
       "      <td>0.950081</td>\n",
       "      <td>0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.051528</td>\n",
       "      <td>0.015798</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'alpha': 0.3}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.884990</td>\n",
       "      <td>0.901010</td>\n",
       "      <td>0.896410</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>35</td>\n",
       "      <td>0.946411</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.945674</td>\n",
       "      <td>0.947932</td>\n",
       "      <td>0.002689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.037565</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.35</td>\n",
       "      <td>{'alpha': 0.35000000000000003}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.901010</td>\n",
       "      <td>0.895684</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>36</td>\n",
       "      <td>0.945344</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.944612</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041222</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.15</td>\n",
       "      <td>{'alpha': 0.15000000000000002}</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.901804</td>\n",
       "      <td>0.895644</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>37</td>\n",
       "      <td>0.947475</td>\n",
       "      <td>0.952859</td>\n",
       "      <td>0.947791</td>\n",
       "      <td>0.949375</td>\n",
       "      <td>0.002467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.037901</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.008643</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.4</td>\n",
       "      <td>{'alpha': 0.4}</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.880626</td>\n",
       "      <td>0.901010</td>\n",
       "      <td>0.894956</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>38</td>\n",
       "      <td>0.945344</td>\n",
       "      <td>0.951710</td>\n",
       "      <td>0.944612</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032579</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'alpha': 0.05}</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.874759</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.893641</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>39</td>\n",
       "      <td>0.948537</td>\n",
       "      <td>0.953908</td>\n",
       "      <td>0.949799</td>\n",
       "      <td>0.950748</td>\n",
       "      <td>0.002293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "26       0.038894      0.004308         0.017951        0.011313        1.35   \n",
       "27       0.032581      0.002618         0.012632        0.003672         1.4   \n",
       "28       0.041221      0.011753         0.009974        0.000814        1.45   \n",
       "30       0.031913      0.002935         0.009973        0.001410        1.55   \n",
       "32       0.041887      0.007236         0.017620        0.009152        1.65   \n",
       "31       0.041890      0.008810         0.009308        0.000469         1.6   \n",
       "35       0.045211      0.010628         0.009642        0.002619         1.8   \n",
       "38       0.023271      0.013164         0.005320        0.002618        1.95   \n",
       "18       0.034575      0.001695         0.009973        0.000814        0.95   \n",
       "25       0.038901      0.006366         0.009636        0.001242         1.3   \n",
       "12       0.040886      0.006465         0.010306        0.000470        0.65   \n",
       "13       0.038563      0.004976         0.009309        0.001242         0.7   \n",
       "14       0.033909      0.002821         0.008645        0.001243        0.75   \n",
       "29       0.033576      0.004017         0.009308        0.000470         1.5   \n",
       "17       0.035239      0.000470         0.008645        0.001245         0.9   \n",
       "34       0.043217      0.006631         0.014627        0.007390        1.75   \n",
       "36       0.041555      0.010936         0.009640        0.001243        1.85   \n",
       "37       0.026597      0.008708         0.006316        0.003290         1.9   \n",
       "20       0.034242      0.001244         0.009642        0.000469        1.05   \n",
       "19       0.034575      0.000940         0.009640        0.000470           1   \n",
       "16       0.036568      0.004485         0.009308        0.000470        0.85   \n",
       "15       0.044552      0.007395         0.010965        0.001417         0.8   \n",
       "33       0.037233      0.004097         0.009309        0.002048         1.7   \n",
       "21       0.036237      0.001696         0.010306        0.000938         1.1   \n",
       "24       0.036895      0.003554         0.009641        0.000470        1.25   \n",
       "11       0.048537      0.009817         0.009309        0.000471         0.6   \n",
       "3        0.054188      0.006003         0.010307        0.000472         0.2   \n",
       "22       0.037233      0.004018         0.009974        0.001629        1.15   \n",
       "23       0.036235      0.000471         0.009311        0.000470         1.2   \n",
       "8        0.048869      0.015536         0.010971        0.000815        0.45   \n",
       "10       0.034907      0.004535         0.010638        0.001694        0.55   \n",
       "4        0.036565      0.001698         0.009642        0.000469        0.25   \n",
       "9        0.042553      0.012332         0.009974        0.001628         0.5   \n",
       "1        0.039227      0.004977         0.008976        0.000815         0.1   \n",
       "5        0.051528      0.015798         0.011636        0.004631         0.3   \n",
       "6        0.037565      0.002351         0.009972        0.001409        0.35   \n",
       "2        0.041222      0.001243         0.007979        0.000814        0.15   \n",
       "7        0.037901      0.003731         0.008643        0.000941         0.4   \n",
       "0        0.032579      0.005543         0.008645        0.000939        0.05   \n",
       "\n",
       "                            params  split0_test_score  split1_test_score  \\\n",
       "26                 {'alpha': 1.35}           0.909091           0.888889   \n",
       "27   {'alpha': 1.4000000000000001}           0.909091           0.888889   \n",
       "28   {'alpha': 1.4500000000000002}           0.909091           0.888889   \n",
       "30                 {'alpha': 1.55}           0.909091           0.886680   \n",
       "32   {'alpha': 1.6500000000000001}           0.909091           0.886680   \n",
       "31                  {'alpha': 1.6}           0.909091           0.886680   \n",
       "35                  {'alpha': 1.8}           0.909091           0.884462   \n",
       "38   {'alpha': 1.9500000000000002}           0.911290           0.882236   \n",
       "18   {'alpha': 0.9500000000000001}           0.905051           0.887574   \n",
       "25                  {'alpha': 1.3}           0.906883           0.887129   \n",
       "12   {'alpha': 0.6500000000000001}           0.903226           0.887574   \n",
       "13   {'alpha': 0.7000000000000001}           0.903226           0.887574   \n",
       "14   {'alpha': 0.7500000000000001}           0.903226           0.887574   \n",
       "29   {'alpha': 1.5000000000000002}           0.906883           0.888889   \n",
       "17   {'alpha': 0.9000000000000001}           0.903226           0.887574   \n",
       "34   {'alpha': 1.7500000000000002}           0.909091           0.882236   \n",
       "36                 {'alpha': 1.85}           0.909091           0.882236   \n",
       "37   {'alpha': 1.9000000000000001}           0.909091           0.882236   \n",
       "20                 {'alpha': 1.05}           0.905051           0.887574   \n",
       "19                  {'alpha': 1.0}           0.905051           0.887574   \n",
       "16   {'alpha': 0.8500000000000001}           0.903226           0.887574   \n",
       "15                  {'alpha': 0.8}           0.903226           0.887574   \n",
       "33   {'alpha': 1.7000000000000002}           0.909091           0.882236   \n",
       "21                  {'alpha': 1.1}           0.905051           0.885827   \n",
       "24   {'alpha': 1.2500000000000002}           0.906883           0.883629   \n",
       "11   {'alpha': 0.6000000000000001}           0.901010           0.885827   \n",
       "3                   {'alpha': 0.2}           0.903226           0.885437   \n",
       "22   {'alpha': 1.1500000000000001}           0.905051           0.883629   \n",
       "23   {'alpha': 1.2000000000000002}           0.905051           0.883629   \n",
       "8                  {'alpha': 0.45}           0.903226           0.884086   \n",
       "10                 {'alpha': 0.55}           0.901010           0.885827   \n",
       "4                  {'alpha': 0.25}           0.903226           0.883268   \n",
       "9                   {'alpha': 0.5}           0.901010           0.884086   \n",
       "1                   {'alpha': 0.1}           0.901408           0.882012   \n",
       "5                   {'alpha': 0.3}           0.903226           0.884990   \n",
       "6   {'alpha': 0.35000000000000003}           0.903226           0.882812   \n",
       "2   {'alpha': 0.15000000000000002}           0.901408           0.883721   \n",
       "7                   {'alpha': 0.4}           0.903226           0.880626   \n",
       "0                  {'alpha': 0.05}           0.901408           0.874759   \n",
       "\n",
       "    split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "26           0.903491         0.900494        0.008519                1   \n",
       "27           0.903491         0.900494        0.008519                1   \n",
       "28           0.903491         0.900494        0.008519                1   \n",
       "30           0.903093         0.899625        0.009476                4   \n",
       "32           0.903093         0.899625        0.009476                4   \n",
       "31           0.903093         0.899625        0.009476                4   \n",
       "35           0.904959         0.899507        0.010771                7   \n",
       "38           0.904959         0.899499        0.012478                8   \n",
       "18           0.905738         0.899454        0.008405                9   \n",
       "25           0.903491         0.899170        0.008626               10   \n",
       "12           0.906504         0.899099        0.008259               11   \n",
       "13           0.906504         0.899099        0.008259               11   \n",
       "14           0.906504         0.899099        0.008259               11   \n",
       "29           0.901235         0.899006        0.007516               14   \n",
       "17           0.905738         0.898844        0.008035               15   \n",
       "34           0.904959         0.898764        0.011809               16   \n",
       "36           0.904959         0.898764        0.011809               16   \n",
       "37           0.904959         0.898764        0.011809               16   \n",
       "20           0.903491         0.898706        0.007897               19   \n",
       "19           0.903491         0.898706        0.007897               19   \n",
       "16           0.904277         0.898358        0.007638               21   \n",
       "15           0.904277         0.898358        0.007638               21   \n",
       "33           0.903093         0.898144        0.011512               23   \n",
       "21           0.903491         0.898124        0.008719               24   \n",
       "24           0.903491         0.898003        0.010258               25   \n",
       "11           0.906504         0.897777        0.008742               26   \n",
       "3            0.903614         0.897425        0.008479               27   \n",
       "22           0.903491         0.897391        0.009752               28   \n",
       "23           0.903491         0.897391        0.009752               28   \n",
       "8            0.904665         0.897325        0.009379               30   \n",
       "10           0.904665         0.897165        0.008155               31   \n",
       "4            0.903614         0.896703        0.009501               32   \n",
       "9            0.904665         0.896585        0.008963               33   \n",
       "1            0.906188         0.896533        0.010452               34   \n",
       "5            0.901010         0.896410        0.008126               35   \n",
       "6            0.901010         0.895684        0.009147               36   \n",
       "2            0.901804         0.895644        0.008432               37   \n",
       "7            0.901010         0.894956        0.010173               38   \n",
       "0            0.904762         0.893641        0.013421               39   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "26            0.942132            0.950655            0.937120   \n",
       "27            0.942132            0.950655            0.937120   \n",
       "28            0.942132            0.950655            0.937120   \n",
       "30            0.942132            0.950655            0.934010   \n",
       "32            0.941176            0.950655            0.934010   \n",
       "31            0.941176            0.950655            0.934010   \n",
       "35            0.940102            0.949597            0.931841   \n",
       "38            0.937945            0.950555            0.931841   \n",
       "18            0.943205            0.951613            0.941414   \n",
       "25            0.942132            0.950655            0.937120   \n",
       "12            0.944276            0.951710            0.943548   \n",
       "13            0.944276            0.951710            0.943548   \n",
       "14            0.943205            0.950655            0.942482   \n",
       "29            0.942132            0.950655            0.936041   \n",
       "17            0.943205            0.951613            0.942482   \n",
       "34            0.941176            0.949597            0.934010   \n",
       "36            0.940102            0.949597            0.931841   \n",
       "37            0.940102            0.949597            0.931841   \n",
       "20            0.943205            0.951613            0.941414   \n",
       "19            0.943205            0.951613            0.941414   \n",
       "16            0.943205            0.951613            0.942482   \n",
       "15            0.943205            0.950655            0.942482   \n",
       "33            0.941176            0.949597            0.934010   \n",
       "21            0.943205            0.951613            0.939271   \n",
       "24            0.942132            0.950655            0.938197   \n",
       "11            0.944276            0.951710            0.943548   \n",
       "3             0.946411            0.953815            0.945674   \n",
       "22            0.943205            0.951613            0.939271   \n",
       "23            0.942132            0.950655            0.938197   \n",
       "8             0.945344            0.951710            0.944612   \n",
       "10            0.945344            0.951710            0.943548   \n",
       "4             0.946411            0.952764            0.945674   \n",
       "9             0.945344            0.951710            0.943548   \n",
       "1             0.948537            0.952859            0.948847   \n",
       "5             0.946411            0.951710            0.945674   \n",
       "6             0.945344            0.951710            0.944612   \n",
       "2             0.947475            0.952859            0.947791   \n",
       "7             0.945344            0.951710            0.944612   \n",
       "0             0.948537            0.953908            0.949799   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "26          0.943302         0.005587  \n",
       "27          0.943302         0.005587  \n",
       "28          0.943302         0.005587  \n",
       "30          0.942266         0.006796  \n",
       "32          0.941947         0.006817  \n",
       "31          0.941947         0.006817  \n",
       "35          0.940513         0.007254  \n",
       "38          0.940114         0.007792  \n",
       "18          0.945411         0.004446  \n",
       "25          0.943302         0.005587  \n",
       "12          0.946511         0.003688  \n",
       "13          0.946511         0.003688  \n",
       "14          0.945447         0.003694  \n",
       "29          0.942942         0.005994  \n",
       "17          0.945767         0.004144  \n",
       "34          0.941594         0.006370  \n",
       "36          0.940513         0.007254  \n",
       "37          0.940513         0.007254  \n",
       "20          0.945411         0.004446  \n",
       "19          0.945411         0.004446  \n",
       "16          0.945767         0.004144  \n",
       "15          0.945447         0.003694  \n",
       "33          0.941594         0.006370  \n",
       "21          0.944696         0.005148  \n",
       "24          0.943661         0.005200  \n",
       "11          0.946511         0.003688  \n",
       "3           0.948633         0.003677  \n",
       "22          0.944696         0.005148  \n",
       "23          0.943661         0.005200  \n",
       "8           0.947222         0.003188  \n",
       "10          0.946868         0.003502  \n",
       "4           0.948283         0.003183  \n",
       "9           0.946868         0.003502  \n",
       "1           0.950081         0.001968  \n",
       "5           0.947932         0.002689  \n",
       "6           0.947222         0.003188  \n",
       "2           0.949375         0.002467  \n",
       "7           0.947222         0.003188  \n",
       "0           0.950748         0.002293  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gcv.cv_results_).sort_values('mean_test_score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain our best alpha parameter of 1.35 and predict on our test set in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8651911468812877"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb=MultinomialNB(alpha=1.35)\n",
    "mnb.fit(X_train,y_train)\n",
    "pred=mnb.predict(X_test)\n",
    "f1_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our f1 score drops from 0.89 to 0.86. The drop is not too significant for an 85% reduction in the dimensionality of our features.<br/>\n",
    "It seems like the first 1000 features are sufficient predictors for our subreddit classification purposes, though we will still use our full set of features for our most optimized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-gram Count Vectorization<a id=2gram></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we investigate how including bigrams into our list of features will affect the predictive capabilities of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=cleaned_all\n",
    "cvec = CountVectorizer(stop_words='english',strip_accents='unicode',max_features=7236,ngram_range=(1, 2)) #set max features to 7236 for a fair comparison to our baseline model.\n",
    "X_train,X_test,y_train,y_test=train_test_split(df.stoptext,df.subreddit,random_state=42,stratify=df.subreddit)\n",
    "X_train=pd.DataFrame(cvec.fit_transform(X_train).todense(),columns=cvec.get_feature_names())\n",
    "X_test=pd.DataFrame(cvec.transform(X_test).todense(),columns=cvec.get_feature_names())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp           646\n",
      "question      451\n",
      "help          389\n",
      "know          388\n",
      "time          350\n",
      "like          333\n",
      "physics       294\n",
      "answer        268\n",
      "problem       267\n",
      "force         256\n",
      "energy        244\n",
      "need          235\n",
      "ve            196\n",
      "equation      190\n",
      "number        186\n",
      "speed         183\n",
      "thanks        183\n",
      "different     176\n",
      "way           173\n",
      "use           172\n",
      "right         172\n",
      "point         167\n",
      "understand    164\n",
      "mass          163\n",
      "velocity      158\n",
      "work          156\n",
      "solve         153\n",
      "light         145\n",
      "field         144\n",
      "possible      136\n",
      "say           134\n",
      "math          131\n",
      "make          130\n",
      "really        130\n",
      "want          127\n",
      "ball          126\n",
      "using         125\n",
      "example       124\n",
      "correct       121\n",
      "object        120\n",
      "gt            120\n",
      "think         119\n",
      "numbers       119\n",
      "wrong         119\n",
      "function      119\n",
      "got           118\n",
      "sure          118\n",
      "given         117\n",
      "calculate     115\n",
      "water         109\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "word_counts = X_train.sum(axis=0)\n",
    "print(word_counts.sort_values(ascending = False).head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the most recurring features, it seems that recurring bigrams are not necessarily common as they do not appear in our top 50 features.<br/>\n",
    "\n",
    "We attempt to list the most recurring bigrams according to the subreddit they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in AskPhysics\n",
    "physcom=X_train.loc[:,(pd.DataFrame(y_train)==1).subreddit.tolist()].sum(axis=0).sort_values(ascending=False)\n",
    "# Most common words in askmath\n",
    "mathcom=X_train.loc[:,(pd.DataFrame(y_train)==0).subreddit.tolist()].sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amp question               17\n",
       "cos sin                    16\n",
       "amp edit                   15\n",
       "amp know                   13\n",
       "correct answer             13\n",
       "amp help                   12\n",
       "appreciate help            10\n",
       "bits bits                  10\n",
       "amp nbsp                    9\n",
       "cos cos                     9\n",
       "answer amp                  9\n",
       "currently working           9\n",
       "buoyant force               8\n",
       "cos theta                   8\n",
       "angular velocity            7\n",
       "constant speed              7\n",
       "chance winning              7\n",
       "amp sure                    7\n",
       "chance getting              6\n",
       "acceleration gravity        6\n",
       "cannon ball                 6\n",
       "cross product               6\n",
       "confidence interval         6\n",
       "dark matter                 6\n",
       "arrow time                  6\n",
       "able help                   6\n",
       "charged particle            6\n",
       "classical mechanics         5\n",
       "car traveling               5\n",
       "decimal places              5\n",
       "                           ..\n",
       "answer appreciate           2\n",
       "consumed products           2\n",
       "contact angle               2\n",
       "better understanding        2\n",
       "container water             2\n",
       "confirm deny                2\n",
       "bits choose                 2\n",
       "conductor radius            2\n",
       "big factor                  2\n",
       "combinations set            2\n",
       "come close                  2\n",
       "come mind                   2\n",
       "big need                    2\n",
       "coming object               2\n",
       "common physics              2\n",
       "bigger speed                2\n",
       "commutative group           2\n",
       "commutative groups          2\n",
       "comparing different         2\n",
       "biggest problem             2\n",
       "completely circular         2\n",
       "binary numbers              2\n",
       "complex eigenvalues         2\n",
       "complexity field            2\n",
       "concentration substance     2\n",
       "concept energy              2\n",
       "bit trouble                 2\n",
       "concise like                2\n",
       "condensed matter            2\n",
       "conducting sphere           2\n",
       "Length: 303, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we extract a sorted list of the most recurring physics bigrams in descending order\n",
    "phys2gram=[x for x in physcom.index.tolist() if re.match(r'\\w+\\s\\w+',x)]\n",
    "# and we mask our original dataframe to return the sum of counts of the bigram from our AskPhysics subreddit\n",
    "physcom[physcom.index.isin(phys2gram)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the most recurring bigrams, it seems that our bigrams are not very helpful as they rarely deal with physics related topics.<br/>\n",
    "The most recurring physics-related bigram is 'buoyant force' with a sum of count of 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amp amp                     38\n",
       "black hole                  27\n",
       "amp thanks                  23\n",
       "black holes                 15\n",
       "amp thank                   13\n",
       "angular momentum            12\n",
       "charge density              11\n",
       "answer question             11\n",
       "alembert principle          10\n",
       "answer key                  10\n",
       "age universe                10\n",
       "center mass                  9\n",
       "answer answer                9\n",
       "amp ve                       8\n",
       "actual price                 8\n",
       "amp need                     7\n",
       "cos amp                      7\n",
       "dark energy                  7\n",
       "ap physics                   7\n",
       "amp example                  6\n",
       "ap calc                      6\n",
       "ball hits                    6\n",
       "amp sin                      6\n",
       "average speed                5\n",
       "circular loop                5\n",
       "correct amp                  5\n",
       "centre mass                  5\n",
       "best way                     5\n",
       "bar labeled                  5\n",
       "amp distance                 5\n",
       "                            ..\n",
       "considering acceleration     2\n",
       "constant said                2\n",
       "constant thanks              2\n",
       "answer change                2\n",
       "angular frequency            2\n",
       "angle work                   2\n",
       "angle vertical               2\n",
       "angle ve                     2\n",
       "complexity class             2\n",
       "completing square            2\n",
       "compensate half              2\n",
       "coil magnetic                2\n",
       "class final                  2\n",
       "classical action             2\n",
       "clock cycles                 2\n",
       "close speed                  2\n",
       "closer look                  2\n",
       "cm atoms                     2\n",
       "cm calculate                 2\n",
       "cm mhz                       2\n",
       "cm tried                     2\n",
       "coins total                  2\n",
       "compactifications string     2\n",
       "collect states               2\n",
       "college creative             2\n",
       "college want                 2\n",
       "colliding balls              2\n",
       "collision using              2\n",
       "come pairs                   2\n",
       "comes contact                2\n",
       "Length: 288, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we extract a sorted list of the most recurring math bigrams in descending order\n",
    "math2gram=[x for x in mathcom.index.tolist() if re.match(r'\\w+\\s\\w+',x)]\n",
    "# and we mask our original dataframe to return the sum of counts of the bigram from our askmath subreddit\n",
    "mathcom[mathcom.index.isin(math2gram)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, many physics related terms appear in our askmath subreddit.<br/>\n",
    "Examples of these terms include *black hole*, *angular momentum* and *charge density*.<br/>\n",
    "Perhaps many of the posters in the askmath subreddit are asking for math help regarding physics questions.<br/>\n",
    "\n",
    "Next, we perform our GridSearch to obtain our best Multinomial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv=GridSearchCV(MultinomialNB(),classifier_model_params['NaiveBayes'],scoring='f1',n_jobs=-1,verbose=1,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 39 candidates, totalling 117 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 117 out of 117 | elapsed:    9.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  , 1.05, 1.1 ,\n",
       "       1.15, 1.2 , 1.25, 1.3 , 1.35, 1.4 , 1.45, 1.5 , 1.55, 1.6 , 1.65,\n",
       "       1.7 , 1.75, 1.8 , 1.85, 1.9 , 1.95])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\chang\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.284904</td>\n",
       "      <td>0.016067</td>\n",
       "      <td>0.054521</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.15</td>\n",
       "      <td>{'alpha': 0.15000000000000002}</td>\n",
       "      <td>0.924901</td>\n",
       "      <td>0.900585</td>\n",
       "      <td>0.925852</td>\n",
       "      <td>0.917112</td>\n",
       "      <td>0.011693</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988967</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.982036</td>\n",
       "      <td>0.987331</td>\n",
       "      <td>0.003834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254985</td>\n",
       "      <td>0.029922</td>\n",
       "      <td>0.054520</td>\n",
       "      <td>0.012466</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'alpha': 0.2}</td>\n",
       "      <td>0.922772</td>\n",
       "      <td>0.900196</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.916890</td>\n",
       "      <td>0.011975</td>\n",
       "      <td>2</td>\n",
       "      <td>0.986961</td>\n",
       "      <td>0.989980</td>\n",
       "      <td>0.981019</td>\n",
       "      <td>0.985987</td>\n",
       "      <td>0.003723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.277257</td>\n",
       "      <td>0.010170</td>\n",
       "      <td>0.041223</td>\n",
       "      <td>0.008553</td>\n",
       "      <td>1.95</td>\n",
       "      <td>{'alpha': 1.9500000000000002}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.916812</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>3</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.978979</td>\n",
       "      <td>0.969093</td>\n",
       "      <td>0.974024</td>\n",
       "      <td>0.004036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.280581</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.051529</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>1.65</td>\n",
       "      <td>{'alpha': 1.6500000000000001}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.916710</td>\n",
       "      <td>0.012310</td>\n",
       "      <td>4</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.971087</td>\n",
       "      <td>0.975016</td>\n",
       "      <td>0.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.268613</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.045213</td>\n",
       "      <td>0.008157</td>\n",
       "      <td>1.7</td>\n",
       "      <td>{'alpha': 1.7000000000000002}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.916710</td>\n",
       "      <td>0.012310</td>\n",
       "      <td>4</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.970120</td>\n",
       "      <td>0.974693</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.276927</td>\n",
       "      <td>0.011866</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>1.5</td>\n",
       "      <td>{'alpha': 1.5000000000000002}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.901186</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.916687</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>6</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.971087</td>\n",
       "      <td>0.975016</td>\n",
       "      <td>0.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.284570</td>\n",
       "      <td>0.010844</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>0.008619</td>\n",
       "      <td>1.55</td>\n",
       "      <td>{'alpha': 1.55}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.901186</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.916687</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>6</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.971087</td>\n",
       "      <td>0.975016</td>\n",
       "      <td>0.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.295210</td>\n",
       "      <td>0.021406</td>\n",
       "      <td>0.057513</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.931238</td>\n",
       "      <td>0.899225</td>\n",
       "      <td>0.918812</td>\n",
       "      <td>0.916433</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>8</td>\n",
       "      <td>0.989980</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.984032</td>\n",
       "      <td>0.988334</td>\n",
       "      <td>0.003070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.284239</td>\n",
       "      <td>0.017522</td>\n",
       "      <td>0.053857</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>1.8</td>\n",
       "      <td>{'alpha': 1.8}</td>\n",
       "      <td>0.925197</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.916101</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>9</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.970120</td>\n",
       "      <td>0.975018</td>\n",
       "      <td>0.004017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.260303</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>1.75</td>\n",
       "      <td>{'alpha': 1.7500000000000002}</td>\n",
       "      <td>0.925197</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.916101</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>9</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.970120</td>\n",
       "      <td>0.974693</td>\n",
       "      <td>0.004047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.267617</td>\n",
       "      <td>0.014482</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>1.6</td>\n",
       "      <td>{'alpha': 1.6}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>11</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.971087</td>\n",
       "      <td>0.975016</td>\n",
       "      <td>0.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.285568</td>\n",
       "      <td>0.021256</td>\n",
       "      <td>0.049533</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>1.9</td>\n",
       "      <td>{'alpha': 1.9000000000000001}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.916095</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>11</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.978979</td>\n",
       "      <td>0.969093</td>\n",
       "      <td>0.974024</td>\n",
       "      <td>0.004036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.258973</td>\n",
       "      <td>0.012334</td>\n",
       "      <td>0.049866</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>1.45</td>\n",
       "      <td>{'alpha': 1.4500000000000002}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.899010</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.915962</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>13</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.975357</td>\n",
       "      <td>0.003345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.280583</td>\n",
       "      <td>0.016067</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.007768</td>\n",
       "      <td>1.4</td>\n",
       "      <td>{'alpha': 1.4000000000000001}</td>\n",
       "      <td>0.927022</td>\n",
       "      <td>0.899010</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.915962</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>13</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.972112</td>\n",
       "      <td>0.975357</td>\n",
       "      <td>0.003345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.277258</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>0.050531</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.35</td>\n",
       "      <td>{'alpha': 0.35000000000000003}</td>\n",
       "      <td>0.922465</td>\n",
       "      <td>0.903353</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>0.915678</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>15</td>\n",
       "      <td>0.981928</td>\n",
       "      <td>0.984894</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.982274</td>\n",
       "      <td>0.002013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.275264</td>\n",
       "      <td>0.032937</td>\n",
       "      <td>0.049201</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'alpha': 0.3}</td>\n",
       "      <td>0.922465</td>\n",
       "      <td>0.903353</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>0.915678</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>15</td>\n",
       "      <td>0.983936</td>\n",
       "      <td>0.986935</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.983623</td>\n",
       "      <td>0.002840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.264957</td>\n",
       "      <td>0.013734</td>\n",
       "      <td>0.053855</td>\n",
       "      <td>0.011489</td>\n",
       "      <td>1.85</td>\n",
       "      <td>{'alpha': 1.85}</td>\n",
       "      <td>0.925197</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.915485</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>17</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>0.978979</td>\n",
       "      <td>0.969093</td>\n",
       "      <td>0.974349</td>\n",
       "      <td>0.004060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.276592</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>0.052196</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>1.35</td>\n",
       "      <td>{'alpha': 1.35}</td>\n",
       "      <td>0.926733</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.915137</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>18</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.979960</td>\n",
       "      <td>0.973081</td>\n",
       "      <td>0.975680</td>\n",
       "      <td>0.003049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.296540</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>0.4</td>\n",
       "      <td>{'alpha': 0.4}</td>\n",
       "      <td>0.920319</td>\n",
       "      <td>0.905138</td>\n",
       "      <td>0.919028</td>\n",
       "      <td>0.914829</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>19</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.983903</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.981615</td>\n",
       "      <td>0.001663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272270</td>\n",
       "      <td>0.039568</td>\n",
       "      <td>0.053857</td>\n",
       "      <td>0.005872</td>\n",
       "      <td>0.25</td>\n",
       "      <td>{'alpha': 0.25}</td>\n",
       "      <td>0.922772</td>\n",
       "      <td>0.898039</td>\n",
       "      <td>0.923387</td>\n",
       "      <td>0.914732</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>20</td>\n",
       "      <td>0.986961</td>\n",
       "      <td>0.988967</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.985309</td>\n",
       "      <td>0.003843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.297870</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.068483</td>\n",
       "      <td>0.008708</td>\n",
       "      <td>1.25</td>\n",
       "      <td>{'alpha': 1.2500000000000002}</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.914426</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>21</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.973081</td>\n",
       "      <td>0.976008</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.256314</td>\n",
       "      <td>0.021406</td>\n",
       "      <td>0.055518</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>1.3</td>\n",
       "      <td>{'alpha': 1.3}</td>\n",
       "      <td>0.924603</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.914426</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>21</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.973081</td>\n",
       "      <td>0.976008</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.267284</td>\n",
       "      <td>0.021683</td>\n",
       "      <td>0.057514</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{'alpha': 1.2000000000000002}</td>\n",
       "      <td>0.924303</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.914326</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>23</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.973081</td>\n",
       "      <td>0.976008</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.273601</td>\n",
       "      <td>0.007114</td>\n",
       "      <td>0.058511</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>1.1</td>\n",
       "      <td>{'alpha': 1.1}</td>\n",
       "      <td>0.924303</td>\n",
       "      <td>0.894632</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.913595</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>24</td>\n",
       "      <td>0.975952</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.973081</td>\n",
       "      <td>0.976658</td>\n",
       "      <td>0.003248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.302191</td>\n",
       "      <td>0.013915</td>\n",
       "      <td>0.060173</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>1.15</td>\n",
       "      <td>{'alpha': 1.1500000000000001}</td>\n",
       "      <td>0.924303</td>\n",
       "      <td>0.894632</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.913595</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>24</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.973081</td>\n",
       "      <td>0.976008</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.257312</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>0.045877</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.45</td>\n",
       "      <td>{'alpha': 0.45}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.917172</td>\n",
       "      <td>0.913366</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>26</td>\n",
       "      <td>0.979920</td>\n",
       "      <td>0.983903</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.981274</td>\n",
       "      <td>0.001859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.275928</td>\n",
       "      <td>0.011866</td>\n",
       "      <td>0.053524</td>\n",
       "      <td>0.009261</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'alpha': 0.7000000000000001}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.912878</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>27</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.982915</td>\n",
       "      <td>0.977023</td>\n",
       "      <td>0.978972</td>\n",
       "      <td>0.002788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.279917</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.042220</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.75</td>\n",
       "      <td>{'alpha': 0.7500000000000001}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.912878</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>27</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.982915</td>\n",
       "      <td>0.977023</td>\n",
       "      <td>0.978972</td>\n",
       "      <td>0.002788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.283906</td>\n",
       "      <td>0.025248</td>\n",
       "      <td>0.056516</td>\n",
       "      <td>0.010936</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'alpha': 0.05}</td>\n",
       "      <td>0.933594</td>\n",
       "      <td>0.887619</td>\n",
       "      <td>0.917323</td>\n",
       "      <td>0.912856</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>29</td>\n",
       "      <td>0.989980</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.985015</td>\n",
       "      <td>0.988662</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.299862</td>\n",
       "      <td>0.014666</td>\n",
       "      <td>0.049201</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'alpha': 0.6500000000000001}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.900794</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.912770</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>30</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.982915</td>\n",
       "      <td>0.977023</td>\n",
       "      <td>0.979298</td>\n",
       "      <td>0.002586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.291220</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.044548</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'alpha': 0.6000000000000001}</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.902584</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.912644</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>31</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.982915</td>\n",
       "      <td>0.977023</td>\n",
       "      <td>0.979298</td>\n",
       "      <td>0.002586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.311166</td>\n",
       "      <td>0.014199</td>\n",
       "      <td>0.057515</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'alpha': 0.55}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.902584</td>\n",
       "      <td>0.917172</td>\n",
       "      <td>0.912641</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>32</td>\n",
       "      <td>0.977956</td>\n",
       "      <td>0.983903</td>\n",
       "      <td>0.978979</td>\n",
       "      <td>0.980279</td>\n",
       "      <td>0.002596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.295874</td>\n",
       "      <td>0.028563</td>\n",
       "      <td>0.056516</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.902584</td>\n",
       "      <td>0.917172</td>\n",
       "      <td>0.912641</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>32</td>\n",
       "      <td>0.978937</td>\n",
       "      <td>0.983903</td>\n",
       "      <td>0.978979</td>\n",
       "      <td>0.980606</td>\n",
       "      <td>0.002331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.272604</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>0.059839</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>1.05</td>\n",
       "      <td>{'alpha': 1.05}</td>\n",
       "      <td>0.920319</td>\n",
       "      <td>0.894632</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.912264</td>\n",
       "      <td>0.012483</td>\n",
       "      <td>34</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.974052</td>\n",
       "      <td>0.977324</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.283243</td>\n",
       "      <td>0.018189</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'alpha': 0.8}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.898608</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.912149</td>\n",
       "      <td>0.009595</td>\n",
       "      <td>35</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.977665</td>\n",
       "      <td>0.002445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.285569</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>0.057179</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.95</td>\n",
       "      <td>{'alpha': 0.9500000000000001}</td>\n",
       "      <td>0.920319</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.911683</td>\n",
       "      <td>0.011764</td>\n",
       "      <td>36</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.974052</td>\n",
       "      <td>0.977324</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.288561</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.049866</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'alpha': 0.9000000000000001}</td>\n",
       "      <td>0.920319</td>\n",
       "      <td>0.895050</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.911683</td>\n",
       "      <td>0.011764</td>\n",
       "      <td>36</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.974052</td>\n",
       "      <td>0.977324</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.297204</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.050199</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1.0}</td>\n",
       "      <td>0.920319</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>0.911672</td>\n",
       "      <td>0.013319</td>\n",
       "      <td>38</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.974052</td>\n",
       "      <td>0.977324</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.282244</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.050864</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.85</td>\n",
       "      <td>{'alpha': 0.8500000000000001}</td>\n",
       "      <td>0.918164</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.911555</td>\n",
       "      <td>0.010434</td>\n",
       "      <td>39</td>\n",
       "      <td>0.976977</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.974052</td>\n",
       "      <td>0.977324</td>\n",
       "      <td>0.002824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "2        0.284904      0.016067         0.054521        0.006582        0.15   \n",
       "3        0.254985      0.029922         0.054520        0.012466         0.2   \n",
       "38       0.277257      0.010170         0.041223        0.008553        1.95   \n",
       "32       0.280581      0.006324         0.051529        0.001245        1.65   \n",
       "33       0.268613      0.012196         0.045213        0.008157         1.7   \n",
       "29       0.276927      0.011866         0.051196        0.007021         1.5   \n",
       "30       0.284570      0.010844         0.047872        0.008619        1.55   \n",
       "1        0.295210      0.021406         0.057513        0.003291         0.1   \n",
       "35       0.284239      0.017522         0.053857        0.003732         1.8   \n",
       "34       0.260303      0.006463         0.053524        0.002351        1.75   \n",
       "31       0.267617      0.014482         0.052858        0.002934         1.6   \n",
       "37       0.285568      0.021256         0.049533        0.001244         1.9   \n",
       "28       0.258973      0.012334         0.049866        0.007852        1.45   \n",
       "27       0.280583      0.016067         0.056848        0.007768         1.4   \n",
       "6        0.277258      0.016266         0.050531        0.006324        0.35   \n",
       "5        0.275264      0.032937         0.049201        0.001695         0.3   \n",
       "36       0.264957      0.013734         0.053855        0.011489        1.85   \n",
       "26       0.276592      0.011234         0.052196        0.003850        1.35   \n",
       "7        0.296540      0.013164         0.053524        0.009436         0.4   \n",
       "4        0.272270      0.039568         0.053857        0.005872        0.25   \n",
       "24       0.297870      0.015285         0.068483        0.008708        1.25   \n",
       "25       0.256314      0.021406         0.055518        0.004772         1.3   \n",
       "23       0.267284      0.021683         0.057514        0.005719         1.2   \n",
       "21       0.273601      0.007114         0.058511        0.001244         1.1   \n",
       "22       0.302191      0.013915         0.060173        0.013267        1.15   \n",
       "8        0.257312      0.005339         0.045877        0.001410        0.45   \n",
       "13       0.275928      0.011866         0.053524        0.009261         0.7   \n",
       "14       0.279917      0.005421         0.042220        0.004771        0.75   \n",
       "0        0.283906      0.025248         0.056516        0.010936        0.05   \n",
       "12       0.299862      0.014666         0.049201        0.000470        0.65   \n",
       "11       0.291220      0.007330         0.044548        0.001242         0.6   \n",
       "10       0.311166      0.014199         0.057515        0.006924        0.55   \n",
       "9        0.295874      0.028563         0.056516        0.010631         0.5   \n",
       "20       0.272604      0.018069         0.059839        0.005340        1.05   \n",
       "15       0.283243      0.018189         0.052857        0.002156         0.8   \n",
       "18       0.285569      0.014620         0.057179        0.004484        0.95   \n",
       "17       0.288561      0.003761         0.049866        0.003549         0.9   \n",
       "19       0.297204      0.001411         0.050199        0.001882           1   \n",
       "16       0.282244      0.006360         0.050864        0.006359        0.85   \n",
       "\n",
       "                            params  split0_test_score  split1_test_score  \\\n",
       "2   {'alpha': 0.15000000000000002}           0.924901           0.900585   \n",
       "3                   {'alpha': 0.2}           0.922772           0.900196   \n",
       "38   {'alpha': 1.9500000000000002}           0.927022           0.899408   \n",
       "32   {'alpha': 1.6500000000000001}           0.927022           0.899408   \n",
       "33   {'alpha': 1.7000000000000002}           0.927022           0.899408   \n",
       "29   {'alpha': 1.5000000000000002}           0.927022           0.901186   \n",
       "30                 {'alpha': 1.55}           0.927022           0.901186   \n",
       "1                   {'alpha': 0.1}           0.931238           0.899225   \n",
       "35                  {'alpha': 1.8}           0.925197           0.899408   \n",
       "34   {'alpha': 1.7500000000000002}           0.925197           0.899408   \n",
       "31                  {'alpha': 1.6}           0.927022           0.899408   \n",
       "37   {'alpha': 1.9000000000000001}           0.927022           0.899408   \n",
       "28   {'alpha': 1.4500000000000002}           0.927022           0.899010   \n",
       "27   {'alpha': 1.4000000000000001}           0.927022           0.899010   \n",
       "6   {'alpha': 0.35000000000000003}           0.922465           0.903353   \n",
       "5                   {'alpha': 0.3}           0.922465           0.903353   \n",
       "36                 {'alpha': 1.85}           0.925197           0.899408   \n",
       "26                 {'alpha': 1.35}           0.926733           0.896825   \n",
       "7                   {'alpha': 0.4}           0.920319           0.905138   \n",
       "4                  {'alpha': 0.25}           0.922772           0.898039   \n",
       "24   {'alpha': 1.2500000000000002}           0.924603           0.896825   \n",
       "25                  {'alpha': 1.3}           0.924603           0.896825   \n",
       "23   {'alpha': 1.2000000000000002}           0.924303           0.896825   \n",
       "21                  {'alpha': 1.1}           0.924303           0.894632   \n",
       "22   {'alpha': 1.1500000000000001}           0.924303           0.894632   \n",
       "8                  {'alpha': 0.45}           0.918164           0.904762   \n",
       "13   {'alpha': 0.7000000000000001}           0.918164           0.900794   \n",
       "14   {'alpha': 0.7500000000000001}           0.918164           0.900794   \n",
       "0                  {'alpha': 0.05}           0.933594           0.887619   \n",
       "12   {'alpha': 0.6500000000000001}           0.918164           0.900794   \n",
       "11   {'alpha': 0.6000000000000001}           0.916000           0.902584   \n",
       "10                 {'alpha': 0.55}           0.918164           0.902584   \n",
       "9                   {'alpha': 0.5}           0.918164           0.902584   \n",
       "20                 {'alpha': 1.05}           0.920319           0.894632   \n",
       "15                  {'alpha': 0.8}           0.918164           0.898608   \n",
       "18   {'alpha': 0.9500000000000001}           0.920319           0.895050   \n",
       "17   {'alpha': 0.9000000000000001}           0.920319           0.895050   \n",
       "19                  {'alpha': 1.0}           0.920319           0.892857   \n",
       "16   {'alpha': 0.8500000000000001}           0.918164           0.896825   \n",
       "\n",
       "    split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "2            0.925852         0.917112        0.011693                1   \n",
       "3            0.927711         0.916890        0.011975                2   \n",
       "38           0.924000         0.916812        0.012368                3   \n",
       "32           0.923695         0.916710        0.012310                4   \n",
       "33           0.923695         0.916710        0.012310                4   \n",
       "29           0.921844         0.916687        0.011163                6   \n",
       "30           0.921844         0.916687        0.011163                6   \n",
       "1            0.918812         0.916433        0.013183                8   \n",
       "35           0.923695         0.916101        0.011819                9   \n",
       "34           0.923695         0.916101        0.011819                9   \n",
       "31           0.921844         0.916095        0.011987               11   \n",
       "37           0.921844         0.916095        0.011987               11   \n",
       "28           0.921844         0.915962        0.012172               13   \n",
       "27           0.921844         0.915962        0.012172               13   \n",
       "6            0.921212         0.915678        0.008730               15   \n",
       "5            0.921212         0.915678        0.008730               15   \n",
       "36           0.921844         0.915485        0.011450               17   \n",
       "26           0.921844         0.915137        0.013101               18   \n",
       "7            0.919028         0.914829        0.006873               19   \n",
       "4            0.923387         0.914732        0.011807               20   \n",
       "24           0.921844         0.914426        0.012496               21   \n",
       "25           0.921844         0.914426        0.012496               21   \n",
       "23           0.921844         0.914326        0.012415               23   \n",
       "21           0.921844         0.913595        0.013446               24   \n",
       "22           0.921844         0.913595        0.013446               24   \n",
       "8            0.917172         0.913366        0.006098               26   \n",
       "13           0.919679         0.912878        0.008567               27   \n",
       "14           0.919679         0.912878        0.008567               27   \n",
       "0            0.917323         0.912856        0.019041               29   \n",
       "12           0.919355         0.912770        0.008482               30   \n",
       "11           0.919355         0.912644        0.007244               31   \n",
       "10           0.917172         0.912641        0.007122               32   \n",
       "9            0.917172         0.912641        0.007122               32   \n",
       "20           0.921844         0.912264        0.012483               34   \n",
       "15           0.919679         0.912149        0.009595               35   \n",
       "18           0.919679         0.911683        0.011764               36   \n",
       "17           0.919679         0.911683        0.011764               36   \n",
       "19           0.921844         0.911672        0.013319               38   \n",
       "16           0.919679         0.911555        0.010434               39   \n",
       "\n",
       "    split0_train_score  split1_train_score  split2_train_score  \\\n",
       "2             0.988967            0.990991            0.982036   \n",
       "3             0.986961            0.989980            0.981019   \n",
       "38            0.974000            0.978979            0.969093   \n",
       "32            0.974000            0.979960            0.971087   \n",
       "33            0.974000            0.979960            0.970120   \n",
       "29            0.974000            0.979960            0.971087   \n",
       "30            0.974000            0.979960            0.971087   \n",
       "1             0.989980            0.990991            0.984032   \n",
       "35            0.974975            0.979960            0.970120   \n",
       "34            0.974000            0.979960            0.970120   \n",
       "31            0.974000            0.979960            0.971087   \n",
       "37            0.974000            0.978979            0.969093   \n",
       "28            0.974000            0.979960            0.972112   \n",
       "27            0.974000            0.979960            0.972112   \n",
       "6             0.981928            0.984894            0.980000   \n",
       "5             0.983936            0.986935            0.980000   \n",
       "36            0.974975            0.978979            0.969093   \n",
       "26            0.974000            0.979960            0.973081   \n",
       "7             0.980943            0.983903            0.980000   \n",
       "4             0.986961            0.988967            0.980000   \n",
       "24            0.974000            0.980943            0.973081   \n",
       "25            0.974000            0.980943            0.973081   \n",
       "23            0.974000            0.980943            0.973081   \n",
       "21            0.975952            0.980943            0.973081   \n",
       "22            0.974000            0.980943            0.973081   \n",
       "8             0.979920            0.983903            0.980000   \n",
       "13            0.976977            0.982915            0.977023   \n",
       "14            0.976977            0.982915            0.977023   \n",
       "0             0.989980            0.990991            0.985015   \n",
       "12            0.977956            0.982915            0.977023   \n",
       "11            0.977956            0.982915            0.977023   \n",
       "10            0.977956            0.983903            0.978979   \n",
       "9             0.978937            0.983903            0.978979   \n",
       "20            0.976977            0.980943            0.974052   \n",
       "15            0.976977            0.980943            0.975075   \n",
       "18            0.976977            0.980943            0.974052   \n",
       "17            0.976977            0.980943            0.974052   \n",
       "19            0.976977            0.980943            0.974052   \n",
       "16            0.976977            0.980943            0.974052   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "2           0.987331         0.003834  \n",
       "3           0.985987         0.003723  \n",
       "38          0.974024         0.004036  \n",
       "32          0.975016         0.003693  \n",
       "33          0.974693         0.004047  \n",
       "29          0.975016         0.003693  \n",
       "30          0.975016         0.003693  \n",
       "1           0.988334         0.003070  \n",
       "35          0.975018         0.004017  \n",
       "34          0.974693         0.004047  \n",
       "31          0.975016         0.003693  \n",
       "37          0.974024         0.004036  \n",
       "28          0.975357         0.003345  \n",
       "27          0.975357         0.003345  \n",
       "6           0.982274         0.002013  \n",
       "5           0.983623         0.002840  \n",
       "36          0.974349         0.004060  \n",
       "26          0.975680         0.003049  \n",
       "7           0.981615         0.001663  \n",
       "4           0.985309         0.003843  \n",
       "24          0.976008         0.003510  \n",
       "25          0.976008         0.003510  \n",
       "23          0.976008         0.003510  \n",
       "21          0.976658         0.003248  \n",
       "22          0.976008         0.003510  \n",
       "8           0.981274         0.001859  \n",
       "13          0.978972         0.002788  \n",
       "14          0.978972         0.002788  \n",
       "0           0.988662         0.002612  \n",
       "12          0.979298         0.002586  \n",
       "11          0.979298         0.002586  \n",
       "10          0.980279         0.002596  \n",
       "9           0.980606         0.002331  \n",
       "20          0.977324         0.002824  \n",
       "15          0.977665         0.002445  \n",
       "18          0.977324         0.002824  \n",
       "17          0.977324         0.002824  \n",
       "19          0.977324         0.002824  \n",
       "16          0.977324         0.002824  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gcv.cv_results_).sort_values('mean_test_score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8875968992248061"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb=MultinomialNB(alpha=0.15)\n",
    "mnb.fit(X_train,y_train)\n",
    "pred=mnb.predict(X_test)\n",
    "f1_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our score drops to 0.88 from 0.89. It is actually better to use only single words as our features in our classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
